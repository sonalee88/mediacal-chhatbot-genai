{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de23aada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ok\n"
     ]
    }
   ],
   "source": [
    "print(\"Ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5a35aca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e:\\\\2025\\\\Generative AI\\\\Langchain Projects\\\\mediacal-chhatbot-genai\\\\research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70080129",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54c89cac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e:\\\\2025\\\\Generative AI\\\\Langchain Projects\\\\mediacal-chhatbot-genai'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0c2899ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain\n",
      "  Downloading langchain-0.3.25-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting langchain-core<1.0.0,>=0.3.58 (from langchain)\n",
      "  Downloading langchain_core-0.3.65-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting langchain-text-splitters<1.0.0,>=0.3.8 (from langchain)\n",
      "  Downloading langchain_text_splitters-0.3.8-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting langsmith<0.4,>=0.1.17 (from langchain)\n",
      "  Downloading langsmith-0.3.45-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic<3.0.0,>=2.7.4 (from langchain)\n",
      "  Downloading pydantic-2.11.7-py3-none-any.whl.metadata (67 kB)\n",
      "Collecting SQLAlchemy<3,>=1.4 (from langchain)\n",
      "  Downloading sqlalchemy-2.0.41-cp310-cp310-win_amd64.whl.metadata (9.8 kB)\n",
      "Collecting requests<3,>=2 (from langchain)\n",
      "  Downloading requests-2.32.4-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting PyYAML>=5.3 (from langchain)\n",
      "  Downloading PyYAML-6.0.2-cp310-cp310-win_amd64.whl.metadata (2.1 kB)\n",
      "Collecting async-timeout<5.0.0,>=4.0.0 (from langchain)\n",
      "  Using cached async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting tenacity!=8.4.0,<10.0.0,>=8.1.0 (from langchain-core<1.0.0,>=0.3.58->langchain)\n",
      "  Downloading tenacity-9.1.2-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<1.0.0,>=0.3.58->langchain)\n",
      "  Using cached jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting packaging<25,>=23.2 (from langchain-core<1.0.0,>=0.3.58->langchain)\n",
      "  Downloading packaging-24.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in d:\\anaconda\\envs\\mediacal-chhatbot-genai\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (4.14.0)\n",
      "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.58->langchain)\n",
      "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from langsmith<0.4,>=0.1.17->langchain)\n",
      "  Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.4,>=0.1.17->langchain)\n",
      "  Downloading orjson-3.10.18-cp310-cp310-win_amd64.whl.metadata (43 kB)\n",
      "Collecting requests-toolbelt<2.0.0,>=1.0.0 (from langsmith<0.4,>=0.1.17->langchain)\n",
      "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Collecting zstandard<0.24.0,>=0.23.0 (from langsmith<0.4,>=0.1.17->langchain)\n",
      "  Downloading zstandard-0.23.0-cp310-cp310-win_amd64.whl.metadata (3.0 kB)\n",
      "Collecting anyio (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain)\n",
      "  Downloading anyio-4.9.0-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting certifi (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain)\n",
      "  Downloading certifi-2025.6.15-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain)\n",
      "  Downloading httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting idna (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain)\n",
      "  Downloading idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting h11>=0.16 (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain)\n",
      "  Downloading h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3.0.0,>=2.7.4->langchain)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic<3.0.0,>=2.7.4->langchain)\n",
      "  Downloading pydantic_core-2.33.2-cp310-cp310-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic<3.0.0,>=2.7.4->langchain)\n",
      "  Downloading typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting charset_normalizer<4,>=2 (from requests<3,>=2->langchain)\n",
      "  Downloading charset_normalizer-3.4.2-cp310-cp310-win_amd64.whl.metadata (36 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests<3,>=2->langchain)\n",
      "  Downloading urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting greenlet>=1 (from SQLAlchemy<3,>=1.4->langchain)\n",
      "  Downloading greenlet-3.2.3-cp310-cp310-win_amd64.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in d:\\anaconda\\envs\\mediacal-chhatbot-genai\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.0)\n",
      "Collecting sniffio>=1.1 (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Downloading langchain-0.3.25-py3-none-any.whl (1.0 MB)\n",
      "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.0/1.0 MB 15.9 MB/s eta 0:00:00\n",
      "Using cached async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Downloading langchain_core-0.3.65-py3-none-any.whl (438 kB)\n",
      "Using cached jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Downloading langchain_text_splitters-0.3.8-py3-none-any.whl (32 kB)\n",
      "Downloading langsmith-0.3.45-py3-none-any.whl (363 kB)\n",
      "Downloading httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Downloading httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Downloading orjson-3.10.18-cp310-cp310-win_amd64.whl (134 kB)\n",
      "Downloading packaging-24.2-py3-none-any.whl (65 kB)\n",
      "Downloading pydantic-2.11.7-py3-none-any.whl (444 kB)\n",
      "Downloading pydantic_core-2.33.2-cp310-cp310-win_amd64.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.0/2.0 MB 21.7 MB/s eta 0:00:00\n",
      "Downloading requests-2.32.4-py3-none-any.whl (64 kB)\n",
      "Downloading charset_normalizer-3.4.2-cp310-cp310-win_amd64.whl (105 kB)\n",
      "Downloading idna-3.10-py3-none-any.whl (70 kB)\n",
      "Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "Downloading sqlalchemy-2.0.41-cp310-cp310-win_amd64.whl (2.1 MB)\n",
      "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "   ----------------------------- ---------- 1.6/2.1 MB 16.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.1/2.1 MB 5.1 MB/s eta 0:00:00\n",
      "Downloading tenacity-9.1.2-py3-none-any.whl (28 kB)\n",
      "Downloading urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
      "Downloading zstandard-0.23.0-cp310-cp310-win_amd64.whl (495 kB)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading certifi-2025.6.15-py3-none-any.whl (157 kB)\n",
      "Downloading greenlet-3.2.3-cp310-cp310-win_amd64.whl (296 kB)\n",
      "Downloading h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Downloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading PyYAML-6.0.2-cp310-cp310-win_amd64.whl (161 kB)\n",
      "Downloading typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Downloading anyio-4.9.0-py3-none-any.whl (100 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Installing collected packages: zstandard, urllib3, typing-inspection, tenacity, sniffio, PyYAML, pydantic-core, packaging, orjson, jsonpointer, idna, h11, greenlet, charset_normalizer, certifi, async-timeout, annotated-types, SQLAlchemy, requests, pydantic, jsonpatch, httpcore, anyio, requests-toolbelt, httpx, langsmith, langchain-core, langchain-text-splitters, langchain\n",
      "\n",
      "   - --------------------------------------  1/29 [urllib3]\n",
      "   - --------------------------------------  1/29 [urllib3]\n",
      "   - --------------------------------------  1/29 [urllib3]\n",
      "   - --------------------------------------  1/29 [urllib3]\n",
      "   -- -------------------------------------  2/29 [typing-inspection]\n",
      "   ---- -----------------------------------  3/29 [tenacity]\n",
      "   ------ ---------------------------------  5/29 [PyYAML]\n",
      "   ------ ---------------------------------  5/29 [PyYAML]\n",
      "  Attempting uninstall: packaging\n",
      "   ------ ---------------------------------  5/29 [PyYAML]\n",
      "    Found existing installation: packaging 25.0\n",
      "   ------ ---------------------------------  5/29 [PyYAML]\n",
      "   --------- ------------------------------  7/29 [packaging]\n",
      "    Uninstalling packaging-25.0:\n",
      "   --------- ------------------------------  7/29 [packaging]\n",
      "      Successfully uninstalled packaging-25.0\n",
      "   --------- ------------------------------  7/29 [packaging]\n",
      "   --------- ------------------------------  7/29 [packaging]\n",
      "   ----------- ----------------------------  8/29 [orjson]\n",
      "   ------------- -------------------------- 10/29 [idna]\n",
      "   --------------- ------------------------ 11/29 [h11]\n",
      "   --------------- ------------------------ 11/29 [h11]\n",
      "   ---------------- ----------------------- 12/29 [greenlet]\n",
      "   ---------------- ----------------------- 12/29 [greenlet]\n",
      "   ---------------- ----------------------- 12/29 [greenlet]\n",
      "   ----------------- ---------------------- 13/29 [charset_normalizer]\n",
      "   ----------------- ---------------------- 13/29 [charset_normalizer]\n",
      "   ------------------- -------------------- 14/29 [certifi]\n",
      "   ----------------------- ---------------- 17/29 [SQLAlchemy]\n",
      "   ----------------------- ---------------- 17/29 [SQLAlchemy]\n",
      "   ----------------------- ---------------- 17/29 [SQLAlchemy]\n",
      "   ----------------------- ---------------- 17/29 [SQLAlchemy]\n",
      "   ----------------------- ---------------- 17/29 [SQLAlchemy]\n",
      "   ----------------------- ---------------- 17/29 [SQLAlchemy]\n",
      "   ----------------------- ---------------- 17/29 [SQLAlchemy]\n",
      "   ----------------------- ---------------- 17/29 [SQLAlchemy]\n",
      "   ----------------------- ---------------- 17/29 [SQLAlchemy]\n",
      "   ----------------------- ---------------- 17/29 [SQLAlchemy]\n",
      "   ----------------------- ---------------- 17/29 [SQLAlchemy]\n",
      "   ----------------------- ---------------- 17/29 [SQLAlchemy]\n",
      "   ----------------------- ---------------- 17/29 [SQLAlchemy]\n",
      "   ----------------------- ---------------- 17/29 [SQLAlchemy]\n",
      "   ----------------------- ---------------- 17/29 [SQLAlchemy]\n",
      "   ----------------------- ---------------- 17/29 [SQLAlchemy]\n",
      "   ----------------------- ---------------- 17/29 [SQLAlchemy]\n",
      "   ----------------------- ---------------- 17/29 [SQLAlchemy]\n",
      "   ----------------------- ---------------- 17/29 [SQLAlchemy]\n",
      "   ----------------------- ---------------- 17/29 [SQLAlchemy]\n",
      "   ----------------------- ---------------- 17/29 [SQLAlchemy]\n",
      "   ----------------------- ---------------- 17/29 [SQLAlchemy]\n",
      "   ----------------------- ---------------- 17/29 [SQLAlchemy]\n",
      "   ----------------------- ---------------- 17/29 [SQLAlchemy]\n",
      "   ----------------------- ---------------- 17/29 [SQLAlchemy]\n",
      "   ----------------------- ---------------- 17/29 [SQLAlchemy]\n",
      "   ----------------------- ---------------- 17/29 [SQLAlchemy]\n",
      "   ----------------------- ---------------- 17/29 [SQLAlchemy]\n",
      "   ----------------------- ---------------- 17/29 [SQLAlchemy]\n",
      "   ----------------------- ---------------- 17/29 [SQLAlchemy]\n",
      "   ----------------------- ---------------- 17/29 [SQLAlchemy]\n",
      "   ----------------------- ---------------- 17/29 [SQLAlchemy]\n",
      "   ----------------------- ---------------- 17/29 [SQLAlchemy]\n",
      "   ----------------------- ---------------- 17/29 [SQLAlchemy]\n",
      "   ----------------------- ---------------- 17/29 [SQLAlchemy]\n",
      "   ----------------------- ---------------- 17/29 [SQLAlchemy]\n",
      "   ----------------------- ---------------- 17/29 [SQLAlchemy]\n",
      "   ----------------------- ---------------- 17/29 [SQLAlchemy]\n",
      "   ----------------------- ---------------- 17/29 [SQLAlchemy]\n",
      "   ------------------------ --------------- 18/29 [requests]\n",
      "   ------------------------ --------------- 18/29 [requests]\n",
      "   -------------------------- ------------- 19/29 [pydantic]\n",
      "   -------------------------- ------------- 19/29 [pydantic]\n",
      "   -------------------------- ------------- 19/29 [pydantic]\n",
      "   -------------------------- ------------- 19/29 [pydantic]\n",
      "   -------------------------- ------------- 19/29 [pydantic]\n",
      "   -------------------------- ------------- 19/29 [pydantic]\n",
      "   -------------------------- ------------- 19/29 [pydantic]\n",
      "   -------------------------- ------------- 19/29 [pydantic]\n",
      "   -------------------------- ------------- 19/29 [pydantic]\n",
      "   -------------------------- ------------- 19/29 [pydantic]\n",
      "   -------------------------- ------------- 19/29 [pydantic]\n",
      "   --------------------------- ------------ 20/29 [jsonpatch]\n",
      "   ---------------------------- ----------- 21/29 [httpcore]\n",
      "   ---------------------------- ----------- 21/29 [httpcore]\n",
      "   ---------------------------- ----------- 21/29 [httpcore]\n",
      "   ------------------------------ --------- 22/29 [anyio]\n",
      "   ------------------------------ --------- 22/29 [anyio]\n",
      "   ------------------------------ --------- 22/29 [anyio]\n",
      "   ------------------------------ --------- 22/29 [anyio]\n",
      "   ------------------------------- -------- 23/29 [requests-toolbelt]\n",
      "   ------------------------------- -------- 23/29 [requests-toolbelt]\n",
      "   ------------------------------- -------- 23/29 [requests-toolbelt]\n",
      "   --------------------------------- ------ 24/29 [httpx]\n",
      "   --------------------------------- ------ 24/29 [httpx]\n",
      "   --------------------------------- ------ 24/29 [httpx]\n",
      "   --------------------------------- ------ 24/29 [httpx]\n",
      "   ---------------------------------- ----- 25/29 [langsmith]\n",
      "   ---------------------------------- ----- 25/29 [langsmith]\n",
      "   ---------------------------------- ----- 25/29 [langsmith]\n",
      "   ---------------------------------- ----- 25/29 [langsmith]\n",
      "   ---------------------------------- ----- 25/29 [langsmith]\n",
      "   ---------------------------------- ----- 25/29 [langsmith]\n",
      "   ---------------------------------- ----- 25/29 [langsmith]\n",
      "   ---------------------------------- ----- 25/29 [langsmith]\n",
      "   ----------------------------------- ---- 26/29 [langchain-core]\n",
      "   ----------------------------------- ---- 26/29 [langchain-core]\n",
      "   ----------------------------------- ---- 26/29 [langchain-core]\n",
      "   ----------------------------------- ---- 26/29 [langchain-core]\n",
      "   ----------------------------------- ---- 26/29 [langchain-core]\n",
      "   ----------------------------------- ---- 26/29 [langchain-core]\n",
      "   ----------------------------------- ---- 26/29 [langchain-core]\n",
      "   ----------------------------------- ---- 26/29 [langchain-core]\n",
      "   ----------------------------------- ---- 26/29 [langchain-core]\n",
      "   ----------------------------------- ---- 26/29 [langchain-core]\n",
      "   ----------------------------------- ---- 26/29 [langchain-core]\n",
      "   ----------------------------------- ---- 26/29 [langchain-core]\n",
      "   ----------------------------------- ---- 26/29 [langchain-core]\n",
      "   ----------------------------------- ---- 26/29 [langchain-core]\n",
      "   ----------------------------------- ---- 26/29 [langchain-core]\n",
      "   ----------------------------------- ---- 26/29 [langchain-core]\n",
      "   ----------------------------------- ---- 26/29 [langchain-core]\n",
      "   ----------------------------------- ---- 26/29 [langchain-core]\n",
      "   ------------------------------------- -- 27/29 [langchain-text-splitters]\n",
      "   ------------------------------------- -- 27/29 [langchain-text-splitters]\n",
      "   -------------------------------------- - 28/29 [langchain]\n",
      "   -------------------------------------- - 28/29 [langchain]\n",
      "   -------------------------------------- - 28/29 [langchain]\n",
      "   -------------------------------------- - 28/29 [langchain]\n",
      "   -------------------------------------- - 28/29 [langchain]\n",
      "   -------------------------------------- - 28/29 [langchain]\n",
      "   -------------------------------------- - 28/29 [langchain]\n",
      "   -------------------------------------- - 28/29 [langchain]\n",
      "   -------------------------------------- - 28/29 [langchain]\n",
      "   -------------------------------------- - 28/29 [langchain]\n",
      "   -------------------------------------- - 28/29 [langchain]\n",
      "   -------------------------------------- - 28/29 [langchain]\n",
      "   -------------------------------------- - 28/29 [langchain]\n",
      "   -------------------------------------- - 28/29 [langchain]\n",
      "   -------------------------------------- - 28/29 [langchain]\n",
      "   -------------------------------------- - 28/29 [langchain]\n",
      "   -------------------------------------- - 28/29 [langchain]\n",
      "   -------------------------------------- - 28/29 [langchain]\n",
      "   -------------------------------------- - 28/29 [langchain]\n",
      "   -------------------------------------- - 28/29 [langchain]\n",
      "   -------------------------------------- - 28/29 [langchain]\n",
      "   -------------------------------------- - 28/29 [langchain]\n",
      "   -------------------------------------- - 28/29 [langchain]\n",
      "   -------------------------------------- - 28/29 [langchain]\n",
      "   -------------------------------------- - 28/29 [langchain]\n",
      "   -------------------------------------- - 28/29 [langchain]\n",
      "   -------------------------------------- - 28/29 [langchain]\n",
      "   -------------------------------------- - 28/29 [langchain]\n",
      "   -------------------------------------- - 28/29 [langchain]\n",
      "   -------------------------------------- - 28/29 [langchain]\n",
      "   -------------------------------------- - 28/29 [langchain]\n",
      "   -------------------------------------- - 28/29 [langchain]\n",
      "   -------------------------------------- - 28/29 [langchain]\n",
      "   -------------------------------------- - 28/29 [langchain]\n",
      "   -------------------------------------- - 28/29 [langchain]\n",
      "   -------------------------------------- - 28/29 [langchain]\n",
      "   -------------------------------------- - 28/29 [langchain]\n",
      "   -------------------------------------- - 28/29 [langchain]\n",
      "   -------------------------------------- - 28/29 [langchain]\n",
      "   -------------------------------------- - 28/29 [langchain]\n",
      "   -------------------------------------- - 28/29 [langchain]\n",
      "   -------------------------------------- - 28/29 [langchain]\n",
      "   -------------------------------------- - 28/29 [langchain]\n",
      "   -------------------------------------- - 28/29 [langchain]\n",
      "   -------------------------------------- - 28/29 [langchain]\n",
      "   -------------------------------------- - 28/29 [langchain]\n",
      "   -------------------------------------- - 28/29 [langchain]\n",
      "   -------------------------------------- - 28/29 [langchain]\n",
      "   -------------------------------------- - 28/29 [langchain]\n",
      "   -------------------------------------- - 28/29 [langchain]\n",
      "   -------------------------------------- - 28/29 [langchain]\n",
      "   -------------------------------------- - 28/29 [langchain]\n",
      "   -------------------------------------- - 28/29 [langchain]\n",
      "   -------------------------------------- - 28/29 [langchain]\n",
      "   -------------------------------------- - 28/29 [langchain]\n",
      "   -------------------------------------- - 28/29 [langchain]\n",
      "   -------------------------------------- - 28/29 [langchain]\n",
      "   -------------------------------------- - 28/29 [langchain]\n",
      "   -------------------------------------- - 28/29 [langchain]\n",
      "   -------------------------------------- - 28/29 [langchain]\n",
      "   -------------------------------------- - 28/29 [langchain]\n",
      "   -------------------------------------- - 28/29 [langchain]\n",
      "   -------------------------------------- - 28/29 [langchain]\n",
      "   -------------------------------------- - 28/29 [langchain]\n",
      "   -------------------------------------- - 28/29 [langchain]\n",
      "   -------------------------------------- - 28/29 [langchain]\n",
      "   -------------------------------------- - 28/29 [langchain]\n",
      "   -------------------------------------- - 28/29 [langchain]\n",
      "   -------------------------------------- - 28/29 [langchain]\n",
      "   -------------------------------------- - 28/29 [langchain]\n",
      "   -------------------------------------- - 28/29 [langchain]\n",
      "   -------------------------------------- - 28/29 [langchain]\n",
      "   -------------------------------------- - 28/29 [langchain]\n",
      "   -------------------------------------- - 28/29 [langchain]\n",
      "   -------------------------------------- - 28/29 [langchain]\n",
      "   -------------------------------------- - 28/29 [langchain]\n",
      "   -------------------------------------- - 28/29 [langchain]\n",
      "   -------------------------------------- - 28/29 [langchain]\n",
      "   -------------------------------------- - 28/29 [langchain]\n",
      "   -------------------------------------- - 28/29 [langchain]\n",
      "   -------------------------------------- - 28/29 [langchain]\n",
      "   -------------------------------------- - 28/29 [langchain]\n",
      "   -------------------------------------- - 28/29 [langchain]\n",
      "   -------------------------------------- - 28/29 [langchain]\n",
      "   -------------------------------------- - 28/29 [langchain]\n",
      "   -------------------------------------- - 28/29 [langchain]\n",
      "   -------------------------------------- - 28/29 [langchain]\n",
      "   -------------------------------------- - 28/29 [langchain]\n",
      "   -------------------------------------- - 28/29 [langchain]\n",
      "   -------------------------------------- - 28/29 [langchain]\n",
      "   -------------------------------------- - 28/29 [langchain]\n",
      "   -------------------------------------- - 28/29 [langchain]\n",
      "   -------------------------------------- - 28/29 [langchain]\n",
      "   -------------------------------------- - 28/29 [langchain]\n",
      "   -------------------------------------- - 28/29 [langchain]\n",
      "   -------------------------------------- - 28/29 [langchain]\n",
      "   -------------------------------------- - 28/29 [langchain]\n",
      "   -------------------------------------- - 28/29 [langchain]\n",
      "   -------------------------------------- - 28/29 [langchain]\n",
      "   -------------------------------------- - 28/29 [langchain]\n",
      "   -------------------------------------- - 28/29 [langchain]\n",
      "   -------------------------------------- - 28/29 [langchain]\n",
      "   -------------------------------------- - 28/29 [langchain]\n",
      "   -------------------------------------- - 28/29 [langchain]\n",
      "   -------------------------------------- - 28/29 [langchain]\n",
      "   -------------------------------------- - 28/29 [langchain]\n",
      "   -------------------------------------- - 28/29 [langchain]\n",
      "   -------------------------------------- - 28/29 [langchain]\n",
      "   -------------------------------------- - 28/29 [langchain]\n",
      "   -------------------------------------- - 28/29 [langchain]\n",
      "   -------------------------------------- - 28/29 [langchain]\n",
      "   -------------------------------------- - 28/29 [langchain]\n",
      "   -------------------------------------- - 28/29 [langchain]\n",
      "   ---------------------------------------- 29/29 [langchain]\n",
      "\n",
      "Successfully installed PyYAML-6.0.2 SQLAlchemy-2.0.41 annotated-types-0.7.0 anyio-4.9.0 async-timeout-4.0.3 certifi-2025.6.15 charset_normalizer-3.4.2 greenlet-3.2.3 h11-0.16.0 httpcore-1.0.9 httpx-0.28.1 idna-3.10 jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.3.25 langchain-core-0.3.65 langchain-text-splitters-0.3.8 langsmith-0.3.45 orjson-3.10.18 packaging-24.2 pydantic-2.11.7 pydantic-core-2.33.2 requests-2.32.4 requests-toolbelt-1.0.0 sniffio-1.3.1 tenacity-9.1.2 typing-inspection-0.4.1 urllib3-2.5.0 zstandard-0.23.0\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cb4fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44555736",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract Data From the PDF File\n",
    "def load_pdf_file(data):\n",
    "    loader= DirectoryLoader(data,\n",
    "                            glob=\"*.pdf\",\n",
    "                            loader_cls=PyPDFLoader)\n",
    "\n",
    "    documents=loader.load()\n",
    "\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090c501f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracted data\n",
    "extracted_data=load_pdf_file(data='Data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47077a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the Data into Text Chunks\n",
    "def text_split(extracted_data):\n",
    "    text_splitter=RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)\n",
    "    text_chunks=text_splitter.split_documents(extracted_data)\n",
    "    return text_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1859c13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#text chunks\n",
    "text_chunks=text_split(extracted_data)\n",
    "print(\"Length of Text Chunks\", len(text_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35e1686",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c6bcf54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download the Embeddings from Hugging Face\n",
    "def download_hugging_face_embeddings():\n",
    "    embeddings=HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13673b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = download_hugging_face_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1145c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_result = embeddings.embed_query(\"Hello world\")\n",
    "print(\"Length\", len(query_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e012e406",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2546dc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "PINECONE_API_KEY=os.environ.get('PINECONE_API_KEY')\n",
    "OPENAI_API_KEY=os.environ.get('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ca22df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone.grpc import PineconeGRPC as Pinecone\n",
    "from pinecone import ServerlessSpec\n",
    "import os\n",
    "\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "\n",
    "index_name = \"medicalbot\"\n",
    "\n",
    "\n",
    "pc.create_index(\n",
    "    name=index_name,\n",
    "    dimension=384, \n",
    "    metric=\"cosine\", \n",
    "    spec=ServerlessSpec(\n",
    "        cloud=\"aws\", \n",
    "        region=\"us-east-1\"\n",
    "    ) \n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e15e382",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72dde852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed each chunk and upsert the embeddings into your Pinecone index.\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "docsearch = PineconeVectorStore.from_documents(\n",
    "    documents=text_chunks,\n",
    "    index_name=index_name,\n",
    "    embedding=embeddings, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a936f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Existing index \n",
    "\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "# Embed each chunk and upsert the embeddings into your Pinecone index.\n",
    "docsearch = PineconeVectorStore.from_existing_index(\n",
    "    index_name=index_name,\n",
    "    embedding=embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3005f455",
   "metadata": {},
   "outputs": [],
   "source": [
    "docsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e94a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = docsearch.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1433e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_docs = retriever.invoke(\"What is Acne?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c702ef61",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84783ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI\n",
    "llm = OpenAI(temperature=0.4, max_tokens=500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d650d25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cc59a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7939314",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = rag_chain.invoke({\"input\": \"what is Acromegaly and gigantism?\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae78cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = rag_chain.invoke({\"input\": \"What is stats?\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadd6f45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mediacal-chhatbot-genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
